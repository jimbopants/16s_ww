{
 "metadata": {
  "name": "",
  "signature": "sha256:820e65cb714fb0df111284364deae6ee0a2c4b8e55dc21b6f4b8bc7be36f0eb7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Imports\n",
      "#from study_1001_networks import open_saved\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import string\n",
      "\n",
      "\n",
      "#(path, data, raw_data, edge_list, node_list, meta_data, connected,\n",
      "#full_data, filtered_edges) = open_saved()\n",
      "\n",
      "#rep_seqs = pd.read_pickle('../study_1001_results/rep_otu_sequences')\n",
      "?pwd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rep_set = '/Users/jimbo/qiime/study_1001/output/dn_otu/rep_set/study_1001_seqs_rep_set.fasta'\n",
      "tax_assignments = '/Users/jimbo/qiime/study_1001/output/dn_otu/uclust_assigned_taxonomy/study_1001_seqs_rep_set_tax_assignments.txt'\n",
      "\n",
      "\n",
      "def load_otu_sequences(rep_set, tax_assignments):\n",
      "    \"\"\" Load files for sequences and names into a dataframe\"\"\"\n",
      "    f = open(rep_set, 'rb')\n",
      "    sequences = f.read()\n",
      "    sequences = sequences.split('\\n')\n",
      "    f.close()\n",
      "    # Names\n",
      "    g = open(tax_assignments,'rb')\n",
      "    seq_names = g.read()\n",
      "    seq_names = seq_names.split('\\n')\n",
      "    g.close()\n",
      "    print len(seq_names)\n",
      "    print len(sequences)\n",
      "    # Extract sequence data I need\n",
      "    # Takes just the \"denovo#\" section of the sequence names (even rows)\n",
      "    seq_IDs = [ s.split(' ')[0][1:] for s in sequences[::2]]\n",
      "    seq_strings = [s for s in sequences[1::2]]\n",
      "    seq_IDs = seq_IDs[:-1]\n",
      "    sequences = dict(zip(seq_IDs, seq_strings))  # Dict of format{ denovo#: 'AAAAT...'}\n",
      "    \n",
      "    # split at tabs and remove any rows that do not conform to data pattern\n",
      "    seq_name_split = [s.split('\\t') for s in seq_names]\n",
      "    seq_name_split = [x for x in seq_name_split if len(x) >1]    \n",
      "    # Get phyla strings and strip species section. \n",
      "    phyla = [ row[1] for row in seq_name_split ]\n",
      "    phyla = [string.split('; s')[0] for string in phyla ]\n",
      "    print len(phyla)\n",
      "    # Create dictionary of format {denovo# : 'k__p__c__...'} \n",
      "    names = [ row[0] for row in seq_name_split ]\n",
      "    phyla_OTU_IDs = dict(zip(names, phyla))\n",
      "    print phyla_OTU_IDs\n",
      "    phyla_set = set(phyla_OTU_IDs.values())\n",
      "    print phyla_set\n",
      "    # Invert dictionary to only have 1 denovo assigned OTU per database aligned OTU\n",
      "    shortened_dict = {}\n",
      "    for taxa in phyla_set:\n",
      "        if taxa not in shortened_dict:\n",
      "            shortened_dict[taxa] = dict_key_from_value(phyla_OTU_IDs,taxa)\n",
      "    print shortened_dict\n",
      "    rep_seqs = pd.DataFrame({'taxa' : shortened_dict.keys(), 'seq_IDs' : shortened_dict.values() })\n",
      "    print rep_seqs.head()\n",
      "    \n",
      "    for key, val in sequences.iteritems():\n",
      "        print key\n",
      "        #print val\n",
      "        rep_seqs.loc[rep_seqs.seq_IDs==key, 'sequence'] = val\n",
      "\n",
      "    return rep_seqs    \n",
      "    \n",
      "\n",
      "repseqs = load_otu_sequences(rep_set, tax_assignments)\n",
      "    \n",
      "def dict_key_from_value(a_dict,a_value):\n",
      "    \"\"\" returns first key that matches a dict value\"\"\"\n",
      "    for key,value in a_dict.iteritems():\n",
      "        if value == a_value:\n",
      "            return key\n",
      "\n",
      "        \n",
      "def taxa_string_split(df, name_column, taxa_cat = [ \"kingdom\", \"phylum\", \"class\", \n",
      "                                                  \"order\", \"family\",\"genus\"], depth=6):\n",
      "    \"\"\" df[name_column] -> df[kingdom, phylum, class...] to desired depth\n",
      "        This assumes you have a dataframe with one (or more) name columns. \n",
      "        By default it splits a taxa string in the QIIME format:\n",
      "        \"k__Bacteria; p__Verrucomicrobia; c__Opitutae; o__Opitutales; f__Opitutaceae;\n",
      "        g__Opitutus; s__\"\n",
      "        into individual columns to make comparisons at many different phylogenetic levels possible.\n",
      "         \"\"\"\n",
      "    for i in range(depth):\n",
      "        splits = df[name_column].str.split().str.get(i)\n",
      "        df[taxa_cat[i]] = splits\n",
      "\n",
      "    return df\n",
      "\n",
      "def hamming_distance(s1, s2):\n",
      "    #Return the Hamming distance between equal-length sequences\n",
      "    if len(s1) != len(s2):\n",
      "        raise ValueError(\"Undefined for sequences of unequal length\")\n",
      "    return (sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2)))/float(len(s1))\n",
      "\n",
      "\n",
      "#df.to_pickle('/Users/jimbo/Desktop/working/study_1001_results/rep_otu_sequences')\n",
      "\n",
      "\n",
      "# We have 1068 unique OTUs in these samples. Each OTU 16s sequence is >97% similar to other sequences\n",
      "# easiest thing is to get a dataframe with a row for each of 1000 unique values and a seq_ID\n",
      "# then add the sequence later.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "126271\n",
        "126271\n",
        "0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{}\n",
        "set([])\n",
        "{}\n",
        "Empty DataFrame\n",
        "Columns: [seq_IDs, taxa]\n",
        "Index: []\n",
        "denovo7357\n"
       ]
      },
      {
       "ename": "TypeError",
       "evalue": "invalid type comparison",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-13-2e0c51e9ffb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mrepseqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_otu_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtax_assignments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdict_key_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-13-2e0c51e9ffb5>\u001b[0m in \u001b[0;36mload_otu_sequences\u001b[0;34m(rep_set, tax_assignments)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m#print val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mrep_seqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrep_seqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_IDs\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sequence'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrep_seqs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jimbo/anaconda/lib/python2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 raise TypeError('Could not compare %s type with Series'\n",
        "\u001b[0;32m/Users/jimbo/anaconda/lib/python2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid type comparison\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: invalid type comparison"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I need to convert the above data into arrays or dictionaries. Then I need to compute hamming/levenschtein distances for each pair of sequences. First I should a) get hamming distance working, b) eliminate sequences with no interaction or not enough data to compute interaction. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print rep_seqs.head()\n",
      "#print type(rep_seqs.sequence[1])\n",
      "print rep_seqs.head()\n",
      "#print hamming_distance(rep_seqs.sequence[0], rep_seqs.sequence[1])\n",
      "rep_seqs = taxa_string_split(rep_seqs, 'taxa', depth=6)\n",
      "#print len(rep_seqs)\n",
      "rep_seqs.to_pickle(\"/Users/jimbo/Desktop/working/study_1001_results/\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "       seq_IDs                                               taxa  \\\n",
        "0   denovo4524  k__Bacteria; p__Bacteroidetes; c__Cytophagia; ...   \n",
        "1   denovo1777  k__Bacteria; p__Firmicutes; c__Bacilli; o__Bac...   \n",
        "2  denovo43640  k__Bacteria; p__Proteobacteria; c__Betaproteob...   \n",
        "3  denovo23997  k__Bacteria; p__Proteobacteria; c__Deltaproteo...   \n",
        "4  denovo58852  k__Bacteria; p__Bacteroidetes; c__VC2_1_Bac22;...   \n",
        "\n",
        "                                            sequence       kingdom  \\\n",
        "0  TACGTAGGTGGCAAGCGTTGTCCGGATTTATTGGGTTTAAAGGGTG...  k__Bacteria;   \n",
        "1  TACGGAGGGGGCTAGCGTTGTTCGGAATTACTGGGCGTAAAGCGCG...  k__Bacteria;   \n",
        "2  TACGTAGGGTGCGAGCGTTAATCGGAATTACTGGGCGTAAAGGGTG...  k__Bacteria;   \n",
        "3  TACGTAGGGTGCAAGCGTTGTTCGGAATCACTGGGCGTAAAGGGCG...  k__Bacteria;   \n",
        "4  TACGGAGGATGCAAGCGTTATCCGGATTCATTGGGTTTAAAGGGTG...  k__Bacteria;   \n",
        "\n",
        "               phylum                    class                 order  \\\n",
        "0   p__Bacteroidetes;           c__Cytophagia;      o__Cytophagales;   \n",
        "1      p__Firmicutes;              c__Bacilli;        o__Bacillales;   \n",
        "2  p__Proteobacteria;   c__Betaproteobacteria;  o__Procabacteriales;   \n",
        "3  p__Proteobacteria;  c__Deltaproteobacteria;           o__Sva0853;   \n",
        "4   p__Bacteroidetes;          c__VC2_1_Bac22;                  o__;   \n",
        "\n",
        "                  family      genus  \n",
        "0  f__[Amoebophilaceae];  g__SC3-56  \n",
        "1     f__Planococcaceae;        g__  \n",
        "2  f__Procabacteriaceae;        g__  \n",
        "3           f__S25_1238;        g__  \n",
        "4                   f__;        g__  \n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I don't remember why I wrote a ton of code here. I think the idea was to figure out which \n",
      "# organisms were interacting on the fly from my interaction network dataframe. \n",
      "\n",
      "# Get OTU names of interest\n",
      "edge_names = edge_list['feature1'].tolist()\n",
      "edge_names2 = edge_list['feature2'].tolist()\n",
      "edge_names = edge_names + edge_names2\n",
      "names_set = set(edge_names)\n",
      "\n",
      "#print edge_names\n",
      "\n",
      "#print len(names_set)\n",
      "\n",
      "sequences_subset = {}\n",
      "phyla_OTU_IDs_subset = {}\n",
      "values = []\n",
      "\n",
      "i=0\n",
      "missing_val = []\n",
      "\n",
      "for key,value in phyla_OTU_IDs.iteritems():\n",
      "    valuestrip = value.replace(' ', '')\n",
      "    if valuestrip in names_set:\n",
      "        if valuestrip not in values:\n",
      "            values.append(valuestrip)\n",
      "            phyla_OTU_IDs_subset[key] = valuestrip\n",
      "    elif valuestrip not in names_set:\n",
      "        if valuestrip not in missing_val:\n",
      "            missing_val.append(valuestrip)\n",
      "            #print valuestrip\n",
      "            i+=1\n",
      "        #pass\n",
      "\n",
      "#print missing_val\n",
      "#print len(missing_val)\n",
      "#print i\n",
      "#print phyla_OTU_IDs_subset\n",
      "\n",
      "# There are 171 names in names_set, \n",
      "# a list of names that came from taking all unique taxa strings from both columns of \n",
      "# edge list, a list of all non-NAN rows in the original ccrepe R output. \n",
      "# There are 80 or 91 missing names in phyla_OTU_IDs_subset which was supposed to just\n",
      "\n",
      "\n",
      "            \n",
      "i=0\n",
      "#for name in names_set:\n",
      "    #if name not in phyla_OTU_IDs_subset.values():\n",
      "        \n",
      "        #print name\n",
      "        \n",
      "#print i\n",
      "#print len(phyla_OTU_IDs_subset)\n",
      "#print len(sequences_subset)\n",
      "#print phyla_OTU_IDs_subset\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    }
   ],
   "metadata": {}
  }
 ]
}