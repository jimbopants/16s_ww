{
 "metadata": {
  "name": "",
  "signature": "sha256:891b95d8e3db05d2029af29417dd4c3cadc00d7f1671c37978cd70eefe0990f4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# To redo my analysis with 16s sequence distance I need genus level network. \n",
      "# My goal is to import my function files and do all of my analysis with a few function calls\n",
      "# end result should be all of the plots I've made before (I could chose a new dataset just to see\n",
      "# if this affects any of my scripts at all. \n",
      "\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "import shared_code.shared_functions as sf\n",
      "\n",
      "\n",
      "genus_data = pd.io.parsers.read_table(\"/Users/jimbo/qiime/study_1001/taxonomy_summaries/study_1001_closed_reference_otu_table_L6.txt\")\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# checking how many of 660 organisms are present in >= 10 samples\n",
      "non_zeros = []\n",
      "for row in genus_data.iterrows():\n",
      "     non_zeros.append(26 - sum(row[1]==0))\n",
      "\n",
      "\n",
      "print sum(x>9 for x in non_zeros)        \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "349\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = pd.io.parsers.read_table('../study_1001_data/s1001_lvl6/ccrepe_results_lvl6', sep = ' ')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First thing to do once I have run QIIME and my ccrepe wrapper/script is to filter the edge list I have down to just edges that aren't NAN values. \n",
      "\n",
      "Project\n",
      "    code\n",
      "        shared code\n",
      "        scripts\n",
      "    data\n",
      "    results\n",
      "        folders by dataset\n",
      "\n",
      "\n",
      "\n",
      "data structures needed for a single run/parameter set:\n",
      "    ccrepe results (edge table basically)\n",
      "    raw dataset (what do I need this for?)\n",
      "    de-nan'ed edge table\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "u'/Users/jimbo/Desktop/working/study_1001_code'"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_node_edge_tables(ccrepe_results, path):\n",
      "    # Creates edge and node dataframes from ccrepe result table and stores them\n",
      "    # at path\n",
      "\n",
      "    ccrepe2 = ccrepe_results.dropna() # eliminate NAs\n",
      "    \n",
      "    node_table = pd.DataFrame( columns = ['taxa'])\n",
      "    node_set = set()\n",
      "    for taxa in ccrepe2.iterrows():\n",
      "        for i in range(2):\n",
      "            if taxa[1][i] not in node_set:\n",
      "                node_set.add(taxa[1][i])\n",
      "                node_series = pd.Series(taxa[1][i], index= ['taxa'])\n",
      "                node_table = node_table.append(node_series, ignore_index = True)\n",
      "    \n",
      "    \n",
      "    return node_set, node_table\n",
      "    \n",
      "    \n",
      "node_set, node_table = make_node_edge_tables(data , '/totally/real')\n",
      "#node_table = sf.taxa_string_split(node_table)\n",
      "\n",
      "def retarded_version():\n",
      "    # Initialize Node table:\n",
      "    node_table = pd.DataFrame( columns = ['taxa', \"av_abundance\",\"edges\"])\n",
      "\n",
      "    \n",
      "    \n",
      "    \n",
      "    index_series = ['taxa', 'name', \"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"av_abundance\",\"edges\"]\n",
      "    \n",
      "    # Make simplified names and taxa columns\n",
      "    for match in data2.iterrows():\n",
      "        row_info = []       # Intiialize rows from df entries\n",
      "        row_info_2 = []\n",
      "        # Get and split match info\n",
      "        str_match_1, str_match_2 = str(match[1][0]), str(match[1][1]) \n",
      "        split_match_1, split_match_2 =  str_match_1.split(\";\"), str_match_2.split(\";\")\n",
      "        \n",
      "        # Create new row entry for dataframe filling taxa columns as well as\n",
      "        # name with lowest level of classification\n",
      "        for item in split_match_1:\n",
      "            if len(item)>3:\n",
      "                row_info.append(item[3:])\n",
      "                name_1 = item\n",
      "            if len(item)<4:\n",
      "                row_info.append(\"unassigned\")\n",
      "        \n",
      "        # Repeat for second node in assocation\n",
      "        for item in split_match_2:\n",
      "            if len(item)>3:\n",
      "                row_info_2.append(item[3:])\n",
      "                name_2 = item\n",
      "            elif len(item)<4:\n",
      "                row_info_2.append(\"unassigned\")\n",
      "    \n",
      "        # If name is not present in name list, append it, then initialize abundance and #\n",
      "        # of edges with 0's in row for now. \n",
      "        if name_1 not in node_list:\n",
      "            node_list.append(name_1)\n",
      "            row_info.append(0)\n",
      "            row_info.append(0)\n",
      "            row_info = [match[1][0]] + [name_1] + row_info\n",
      "            series_row = pd.Series(row_info, index = index_series)\n",
      "            node_table = node_table.append( series_row, ignore_index=True)\n",
      "        if name_2 not in node_list:\n",
      "            node_list.append(name_2)\n",
      "            row_info_2.append(0)\n",
      "            row_info_2.append(0)\n",
      "            row_info_2 = [match[1][1]]+ [name_2] + row_info_2\n",
      "            series_row = pd.Series(row_info_2, index = index_series)\n",
      "            node_table = node_table.append( series_row, ignore_index=True)\n",
      "    \n",
      "    # Filter edges based on Q and R\n",
      "    edge_table = data2[data2['q.value'] <.05]\n",
      "    edge_table_pos = edge_table[edge_table['sim.score']>.80]\n",
      "    print edge_table_pos[\"feature1\"]\n",
      "    print node_table[\"taxa\"]\n",
      "    # Convert edge_table names to match with node_table names.\n",
      "    # This code is now almost identically replicated 3 times in this file, in\n",
      "    # the future, don't write code like this. \n",
      "    for edge in edge_table_pos.iterrows():\n",
      "        node_1,node_2 = str(edge[1][0]), str(edge[1][1])\n",
      "        split_node_1, split_node_2 = node_1.split(\";\"), node_2.split(\";\")\n",
      "    \n",
      "        for item in reversed(split_node_1):\n",
      "            if len(item)>3:\n",
      "                name_1 = item\n",
      "            break\n",
      "        for item in reversed(split_node_2):\n",
      "            if len(item)>3:\n",
      "                name_2 = item\n",
      "            break\n",
      "        \n",
      "        #edge_table_pos[\"feature1\"][edge_table_pos[\"feature1\"] == edge[1][0]] = name_1\n",
      "        #edge_table_pos[\"feature2\"][edge_table_pos[\"feature2\"] == edge[1][1]] = name_2\n",
      "\n",
      "    for nodes in edge_table_pos.iterrows():\n",
      "        node_table[\"edges\"][node_table[\"taxa\"] == nodes[1][0]] +=1\n",
      "\n",
      "    store_df(node_table, path, \"s1001_nodes_2\")\n",
      "    store_df(edge_table, path, \"s1001_edges_2\")\n",
      "    return\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print node_table.ix[0]\n",
      "node_taxa_table = taxa_string_split(node_table, 'taxa')\n",
      "print node_taxa_table.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "taxa       k__Archaea;p__Crenarchaeota;c__Thaumarchaeota;...\n",
        "kingdom                                           k__Archaea\n",
        "phylum                                      p__Crenarchaeota\n",
        "class                                      c__Thaumarchaeota\n",
        "order                                   o__Nitrososphaerales\n",
        "family                                 f__Nitrososphaeraceae\n",
        "Name: 0, dtype: object\n",
        "                                                taxa      kingdom  \\\n",
        "0  k__Archaea;p__Crenarchaeota;c__Thaumarchaeota;...   k__Archaea   \n",
        "1       k__Bacteria;p__Acidobacteria;c__;o__;f__;g__  k__Bacteria   \n",
        "2  k__Bacteria;p__Acidobacteria;c__;o__;f__Koriba...  k__Bacteria   \n",
        "3  k__Bacteria;p__Acidobacteria;c__Acidobacteria ...  k__Bacteria   \n",
        "4  k__Bacteria;p__Acidobacteria;c__Acidobacteria ...  k__Bacteria   \n",
        "\n",
        "             phylum                     class                 order  \\\n",
        "0  p__Crenarchaeota         c__Thaumarchaeota  o__Nitrososphaerales   \n",
        "1  p__Acidobacteria                       c__                   o__   \n",
        "2  p__Acidobacteria                       c__                   o__   \n",
        "3  p__Acidobacteria  c__Acidobacteria (class)   o__Acidobacteriales   \n",
        "4  p__Acidobacteria  c__Acidobacteria (class)   o__Acidobacteriales   \n",
        "\n",
        "                  family  \n",
        "0  f__Nitrososphaeraceae  \n",
        "1                    f__  \n",
        "2     f__Koribacteraceae  \n",
        "3                    f__  \n",
        "4   f__Acidobacteriaceae  \n"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def taxa_string_split(df, name_column, sep=';', depth=6):\n",
      "    \"\"\" df[name_column] -> df[kingdom, phylum, class...] to desired depth\n",
      "    This assumes you have a dataframe with one (or more) name columns. \n",
      "    By default it splits a taxa string in the QIIME format:\n",
      "    \"k__Bacteria; p__Verrucomicrobia; c__Opitutae; o__Opitutales; f__Opitutaceae;\n",
      "    g__Opitutus; s__\"\n",
      "    into individual columns to make comparisons at many different phylogenetic levels possible.\n",
      "    \"\"\"\n",
      "    taxa_cat = ['kingdom', 'phylum', 'class', 'order', 'family', 'genus']\n",
      "    for i in range(depth):\n",
      "        splits = df[name_column].str.split(';').str.get(i)\n",
      "        df[taxa_cat[i]] = splits\n",
      "\n",
      "    return df\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}