{
 "metadata": {
  "name": "",
  "signature": "sha256:6cbc3d750c05e271b5c98d8ad545cd4e787c3da4e476b5913044573fcd4fc138"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# To redo my analysis with 16s sequence distance I need genus level network. \n",
      "# My goal is to import my function files and do all of my analysis with a few function calls\n",
      "# end result should be all of the plots I've made before (I could chose a new dataset just to see\n",
      "# if this affects any of my scripts at all. \n",
      "\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "import shared_code.shared_functions as sf\n",
      "\n",
      "\n",
      "genus_data = pd.io.parsers.read_table(\"/Users/jimbo/qiime/study_1001/taxonomy_summaries/study_1001_closed_reference_otu_table_L6.txt\")\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# checking how many of 660 organisms are present in >= 10 samples\n",
      "non_zeros = []\n",
      "for row in genus_data.iterrows():\n",
      "     non_zeros.append(26 - sum(row[1]==0))\n",
      "\n",
      "\n",
      "print sum(x>9 for x in non_zeros)        \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "349\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = pd.io.parsers.read_table('../study_1001_data/s1001_lvl6/ccrepe_results_lvl6', sep = ' ')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First thing to do once I have run QIIME and my ccrepe wrapper/script is to filter the edge list I have down to just edges that aren't NAN values. \n",
      "\n",
      "Project\n",
      "    code\n",
      "        shared code\n",
      "        scripts\n",
      "    data\n",
      "    results\n",
      "        folders by dataset\n",
      "\n",
      "\n",
      "\n",
      "data structures needed for a single run/parameter set:\n",
      "    ccrepe results (edge table basically)\n",
      "    raw dataset (what do I need this for?)\n",
      "    de-nan'ed edge table\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "u'/Users/jimbo/Desktop/working/study_1001_code'"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_node_edge_tables(ccrepe_results, path):\n",
      "    # Creates edge and node dataframes from ccrepe result table and stores them\n",
      "\n",
      "    ccrepe2 = ccrepe_results.dropna() # eliminate NAs\n",
      "    \n",
      "    # initialize a dataframe and add rows for each unique taxon in dataset.\n",
      "    node_table = pd.DataFrame( columns = ['taxa'])\n",
      "    node_set = set()\n",
      "    for taxa in ccrepe2.iterrows():\n",
      "        for i in range(2):\n",
      "            if taxa[1][i] not in node_set:\n",
      "                node_set.add(taxa[1][i])\n",
      "                node_series = pd.Series(taxa[1][i], index= ['taxa'])\n",
      "                node_table = node_table.append(node_series, ignore_index = True)\n",
      "    \n",
      "    return ccrepe2, node_table\n",
      "    \n",
      "edge_table, node_table = make_node_edge_tables(data , '/totally/real')\n",
      "#node_table = sf.taxa_string_split(node_table)\n",
      "\n",
      "def retarded_version():\n",
      "    # Initialize Node table:\n",
      "    node_table = pd.DataFrame( columns = ['taxa', \"av_abundance\",\"edges\"])\n",
      "\n",
      "    \n",
      "    \n",
      "    \n",
      "    index_series = ['taxa', 'name', \"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"av_abundance\",\"edges\"]\n",
      "    \n",
      "    # Make simplified names and taxa columns\n",
      "    for match in data2.iterrows():\n",
      "        row_info = []       # Intiialize rows from df entries\n",
      "        row_info_2 = []\n",
      "        # Get and split match info\n",
      "        str_match_1, str_match_2 = str(match[1][0]), str(match[1][1]) \n",
      "        split_match_1, split_match_2 =  str_match_1.split(\";\"), str_match_2.split(\";\")\n",
      "        \n",
      "        # Create new row entry for dataframe filling taxa columns as well as\n",
      "        # name with lowest level of classification\n",
      "        for item in split_match_1:\n",
      "            if len(item)>3:\n",
      "                row_info.append(item[3:])\n",
      "                name_1 = item\n",
      "            if len(item)<4:\n",
      "                row_info.append(\"unassigned\")\n",
      "        \n",
      "        # Repeat for second node in assocation\n",
      "        for item in split_match_2:\n",
      "            if len(item)>3:\n",
      "                row_info_2.append(item[3:])\n",
      "                name_2 = item\n",
      "            elif len(item)<4:\n",
      "                row_info_2.append(\"unassigned\")\n",
      "    \n",
      "        # If name is not present in name list, append it, then initialize abundance and #\n",
      "        # of edges with 0's in row for now. \n",
      "        if name_1 not in node_list:\n",
      "            node_list.append(name_1)\n",
      "            row_info.append(0)\n",
      "            row_info.append(0)\n",
      "            row_info = [match[1][0]] + [name_1] + row_info\n",
      "            series_row = pd.Series(row_info, index = index_series)\n",
      "            node_table = node_table.append( series_row, ignore_index=True)\n",
      "        if name_2 not in node_list:\n",
      "            node_list.append(name_2)\n",
      "            row_info_2.append(0)\n",
      "            row_info_2.append(0)\n",
      "            row_info_2 = [match[1][1]]+ [name_2] + row_info_2\n",
      "            series_row = pd.Series(row_info_2, index = index_series)\n",
      "            node_table = node_table.append( series_row, ignore_index=True)\n",
      "    \n",
      "    # Filter edges based on Q and R\n",
      "    edge_table = data2[data2['q.value'] <.05]\n",
      "    edge_table_pos = edge_table[edge_table['sim.score']>.80]\n",
      "    print edge_table_pos[\"feature1\"]\n",
      "    print node_table[\"taxa\"]\n",
      "    # Convert edge_table names to match with node_table names.\n",
      "    # This code is now almost identically replicated 3 times in this file, in\n",
      "    # the future, don't write code like this. \n",
      "    for edge in edge_table_pos.iterrows():\n",
      "        node_1,node_2 = str(edge[1][0]), str(edge[1][1])\n",
      "        split_node_1, split_node_2 = node_1.split(\";\"), node_2.split(\";\")\n",
      "    \n",
      "        for item in reversed(split_node_1):\n",
      "            if len(item)>3:\n",
      "                name_1 = item\n",
      "            break\n",
      "        for item in reversed(split_node_2):\n",
      "            if len(item)>3:\n",
      "                name_2 = item\n",
      "            break\n",
      "        \n",
      "        #edge_table_pos[\"feature1\"][edge_table_pos[\"feature1\"] == edge[1][0]] = name_1\n",
      "        #edge_table_pos[\"feature2\"][edge_table_pos[\"feature2\"] == edge[1][1]] = name_2\n",
      "\n",
      "    for nodes in edge_table_pos.iterrows():\n",
      "        node_table[\"edges\"][node_table[\"taxa\"] == nodes[1][0]] +=1\n",
      "\n",
      "    store_df(node_table, path, \"s1001_nodes_2\")\n",
      "    store_df(edge_table, path, \"s1001_edges_2\")\n",
      "    return\n",
      "\n",
      "\n",
      "print node_table\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                                                  taxa\n",
        "0    k__Archaea;p__Crenarchaeota;c__Thaumarchaeota;...\n",
        "1         k__Bacteria;p__Acidobacteria;c__;o__;f__;g__\n",
        "2    k__Bacteria;p__Acidobacteria;c__;o__;f__Koriba...\n",
        "3    k__Bacteria;p__Acidobacteria;c__Acidobacteria ...\n",
        "4    k__Bacteria;p__Acidobacteria;c__Acidobacteria ...\n",
        "5    k__Bacteria;p__Acidobacteria;c__Acidobacteria-...\n",
        "6    k__Bacteria;p__Acidobacteria;c__Chloracidobact...\n",
        "7    k__Bacteria;p__Acidobacteria;c__Solibacteres;o...\n",
        "8    k__Bacteria;p__Acidobacteria;c__Sva0725;o__;f_...\n",
        "9    k__Bacteria;p__Acidobacteria;c__iii1-8;o__;f__...\n",
        "10       k__Bacteria;p__Actinobacteria;c__;o__;f__;g__\n",
        "11   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "12   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "13   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "14   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "15   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "16   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "17   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "18   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "19   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "20   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "21   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "22   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "23   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "24   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "25   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "26   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "27   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "28   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "29   k__Bacteria;p__Actinobacteria;c__Actinobacteri...\n",
        "..                                                 ...\n",
        "349  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "350  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "351  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "352  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "353  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "354  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "355  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "356  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "357  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "358  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "359  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "360  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "361  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "362  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "363  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "364  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "365  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "366  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "367  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "368  k__Bacteria;p__Proteobacteria;c__Gammaproteoba...\n",
        "369  k__Bacteria;p__Thermi;c__Deinococci;o__Deinoco...\n",
        "370  k__Bacteria;p__Verrucomicrobia;c__Opitutae;o__...\n",
        "371  k__Bacteria;p__Verrucomicrobia;c__Opitutae;o__...\n",
        "372  k__Bacteria;p__Verrucomicrobia;c__Spartobacter...\n",
        "373  k__Bacteria;p__Verrucomicrobia;c__Verrucomicro...\n",
        "374  k__Bacteria;p__Verrucomicrobia;c__Verrucomicro...\n",
        "375  k__Bacteria;p__Verrucomicrobia;c__Verrucomicro...\n",
        "376  k__Bacteria;p__Verrucomicrobia;c__Verrucomicro...\n",
        "377  k__Bacteria;p__Verrucomicrobia;c__Verrucomicro...\n",
        "378               k__Bacteria;p__WPS-2;c__;o__;f__;g__\n",
        "\n",
        "[379 rows x 1 columns]\n"
       ]
      }
     ],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print node_table.ix[0]\n",
      "node_taxa_table = taxa_string_split(node_table, 'taxa')\n",
      "print node_taxa_table.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "taxa    k__Archaea;p__Crenarchaeota;c__Thaumarchaeota;...\n",
        "Name: 0, dtype: object\n",
        "                                                taxa      kingdom  \\\n",
        "0  k__Archaea;p__Crenarchaeota;c__Thaumarchaeota;...   k__Archaea   \n",
        "1       k__Bacteria;p__Acidobacteria;c__;o__;f__;g__  k__Bacteria   \n",
        "2  k__Bacteria;p__Acidobacteria;c__;o__;f__Koriba...  k__Bacteria   \n",
        "3  k__Bacteria;p__Acidobacteria;c__Acidobacteria ...  k__Bacteria   \n",
        "4  k__Bacteria;p__Acidobacteria;c__Acidobacteria ...  k__Bacteria   \n",
        "\n",
        "             phylum                     class                 order  \\\n",
        "0  p__Crenarchaeota         c__Thaumarchaeota  o__Nitrososphaerales   \n",
        "1  p__Acidobacteria                       c__                   o__   \n",
        "2  p__Acidobacteria                       c__                   o__   \n",
        "3  p__Acidobacteria  c__Acidobacteria (class)   o__Acidobacteriales   \n",
        "4  p__Acidobacteria  c__Acidobacteria (class)   o__Acidobacteriales   \n",
        "\n",
        "                  family                         genus  \n",
        "0  f__Nitrososphaeraceae  g__Candidatus Nitrososphaera  \n",
        "1                    f__                           g__  \n",
        "2     f__Koribacteraceae      g__Candidatus Koribacter  \n",
        "3                    f__                           g__  \n",
        "4   f__Acidobacteriaceae                           g__  \n"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def taxa_string_split(df, name_column, sep=';', depth=6):\n",
      "    \"\"\" df[name_column] -> df[kingdom, phylum, class...] to desired depth\n",
      "    This assumes you have a dataframe with one (or more) name columns. \n",
      "    By default it splits a taxa string in the QIIME format:\n",
      "    \"k__Bacteria; p__Verrucomicrobia; c__Opitutae; o__Opitutales; f__Opitutaceae;\n",
      "    g__Opitutus; s__\"\n",
      "    into individual columns to make comparisons at many different phylogenetic levels possible.\n",
      "    \"\"\"\n",
      "    taxa_cat = ['kingdom', 'phylum', 'class', 'order', 'family', 'genus']\n",
      "    for i in range(depth):\n",
      "        splits = df[name_column].str.split(';').str.get(i)\n",
      "        df[taxa_cat[i]] = splits\n",
      "\n",
      "    return df\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}